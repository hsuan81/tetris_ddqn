{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "3e088f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, math, glob\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "# from utils.wrappers import *\n",
    "# from agents.DQN import Model as DQN_Agent\n",
    "# from utils.ReplayMemory import ExperienceReplayMemory\n",
    "\n",
    "#from utils.Replay.ipynb import ReplayBuffer\n",
    "from utils.hyperparameters import Config\n",
    "from utils.plot import plot_all_data\n",
    "import Game.tetris_fun as game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "71ae2726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for error: no available video device\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "1490f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, size, screen_shape=(84, 84)):\n",
    "        self.size = size\n",
    "        self.screen_shape = screen_shape\n",
    "        self.num_in_buffer = 0\n",
    "        self.screens = deque(maxlen=self.size)\n",
    "        self.actions = deque(maxlen=self.size)\n",
    "        self.rewards = deque(maxlen=self.size)\n",
    "#          self.next_screens = deque(maxlin=self.size)\n",
    "        self.terminal = deque(maxlen=self.size)\n",
    "        \n",
    "    def push(self, screen, action, reward, terminal):\n",
    "        self.screens.append(screen)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.terminal.append(terminal)\n",
    "#         self.next_screens.append(next_screens)\n",
    "        \n",
    "        self.num_in_buffer = len(self.screens)\n",
    "#         print(\"buffer num \", self.num_in_buffer)\n",
    "#         print(\"action num \", len(self.actions))\n",
    "#         print(\"reward num \", len(self.rewards))\n",
    "#         print(\"ter num \", len(self.terminal))\n",
    "#         print(self.actions)\n",
    "        \n",
    "    def can_sample(self, batch_size):\n",
    "        \"\"\"Returns true if `batch_size` different transitions can be sampled from the buffer.\"\"\"\n",
    "        return batch_size + 1 <= self.num_in_buffer\n",
    "    \n",
    "    def _encode_sample(self, idxes):\n",
    "        # Return batch data for screens, actions, rewards, next screens and terminal info\n",
    "        # one screen state corresponding to one action by default, needing to consider grouped screens and actions\n",
    "        obs_batch      = torch.from_numpy(np.concatenate([self.screens[idx] for idx in idxes], 0))\n",
    "        act_batch      = torch.Tensor([self.actions[idx] for idx in idxes])\n",
    "        rew_batch      = torch.Tensor([self.rewards[idx] for idx in idxes])\n",
    "        next_obs_batch = torch.from_numpy(np.concatenate([self.screens[idx + 1] for idx in idxes], 0))\n",
    "        done_mask      = np.array([1.0 if self.terminal[idx] else 0.0 for idx in idxes], dtype=np.float32)\n",
    "        \n",
    "        return obs_batch, act_batch, rew_batch, next_obs_batch, done_mask\n",
    "        \n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "#         assert self.can_sample(batch_size)\n",
    "        inds = random.sample(range(self.num_in_buffer), batch_size)\n",
    "        print(\"ind \", inds)\n",
    "        \n",
    "        return self._encode_sample(inds)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "b83b52ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([1, 32, 20, 20])\n",
      "[  1.   2. 255.   3.   8. 255.]\n"
     ]
    }
   ],
   "source": [
    "# The DQN model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions, in_channels=3, screen_shape=(84, 84)):\n",
    "        super(DQN, self).__init__()\n",
    "        in_channels = in_channels\n",
    "        num_actions = num_actions\n",
    "        screen_shape = screen_shape\n",
    "        h = screen_shape[0]\n",
    "        w = screen_shape[1]\n",
    "        \n",
    "        \n",
    "        # could add batchnorm2d layers after each covnet if data volume is too large\n",
    "        # see: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        convw = self.conv2d_size_out(self.conv2d_size_out(self.conv2d_size_out(w, 8, 4), 4, 2), 3, 1)\n",
    "        convh = self.conv2d_size_out(self.conv2d_size_out(self.conv2d_size_out(h, 8, 4), 4, 2), 3, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(convw * convh * 64, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        print(x.size())\n",
    "        x = F.relu(x)\n",
    "#         x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # reshape the tensor to one dimension for fc layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def conv2d_size_out(self, size, kernel_size, stride, padding_size=0):\n",
    "        return (size + 2 * padding_size - (kernel_size - 1) - 1) // stride  + 1\n",
    "    \n",
    "\n",
    "m = DQN(6, 1, (84, 84))\n",
    "input1 = torch.randn(1, 1, 84, 84)\n",
    "print(input1.dtype)\n",
    "output = m(input1)\n",
    "\n",
    "x_t = np.array([1, 2, 255, 3, 8, 255], dtype=np.double)\n",
    "# x_t = np.array([0.0 if x//255 < 1 else 255.0 for x in x_t])\n",
    "print(x_t)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "149448e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and utilities\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.001\n",
    "TARGET_UPDATE = 10\n",
    "lr = 0.001\n",
    "memory_size = 100000\n",
    "num_episodes = 1000\n",
    "\n",
    "\n",
    "def get_action(state, policy_net):\n",
    "    # Return a number indicating the pos of 1 in the array for a action\n",
    "    steps_done = 0\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            action = policy_net(state)\n",
    "            print(\"act size\", action.size())\n",
    "            print(action)\n",
    "            action = action.max(1)[1][0]\n",
    "            print(\"act num \", action.size())\n",
    "            return action.item()\n",
    "    else:\n",
    "        action = random.randint(0, 5)\n",
    "        return action\n",
    "    \n",
    "def get_act_array(act_num):\n",
    "    action = np.zeros(6, dtype=int)\n",
    "    action[act_num] = 1\n",
    "    return action\n",
    "\n",
    "def get_next_qs(target_net, next_obs_batch, done_mask, BATCH_SIZE):\n",
    "    not_terminal = np.where(done_mask==0.0)\n",
    "    not_terminal_states = next_obs_batch[not_terminal]\n",
    "    values = torch.zeros(BATCH_SIZE)\n",
    "    values[not_terminal] = target_net(not_terminal_states).max(dim=1)[0].detach()\n",
    "    return values\n",
    "            \n",
    "def preprocess(image, shape=(84,84)):\n",
    "    pass\n",
    "\n",
    "def plot_durations(episode_durations, average_num=100):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= average_num:\n",
    "        means = durations_t.unfold(0, average_num, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(average_num-1), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "#     if is_ipython:\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "daee1396",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.001\n",
    "TARGET_UPDATE = 10\n",
    "lr = 0.001\n",
    "memory_size = 100000\n",
    "num_episodes = 1000\"\"\"\n",
    "\n",
    "\n",
    "def train(env, num_actions, in_channels, memory_size=100000, screen_shape=(84, 84), target_update=10, \n",
    "          BATCH_SIZE=128, GAMMA=0.999, EPS_START=0.9, EPS_END=0.05, EPS_DECAY=0.001, lr=0.001, num_episodes=1000):\n",
    "    env = env\n",
    "    \n",
    "    # if GPU is available, use it otherwise use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    policy_net = DQN(num_actions, in_channels).to(device)\n",
    "    target_net = DQN(num_actions, in_channels).to(device)\n",
    "    \n",
    "    # set weight and bias of target net as policy net\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    \n",
    "    optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "    \n",
    "    memory = ReplayBuffer(memory_size, screen_shape)\n",
    "    \n",
    "    episode_durations = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        start_act = np.zeros(num_actions)\n",
    "        x_t, r_0, terminal = env.frame_step(start_act)\n",
    "        \n",
    "#         x_t = cv2.cvtColor(cv2.resize(x_t, (84, 84)), cv2.COLOR_BGR2GRAY)  # resize the screen and convert color to gray\n",
    "#         ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)  # set the background to black and tetriminos to white\n",
    "# #         s_t = np.stack((x_t, x_t, x_t, x_t), axis = 2)\n",
    "\n",
    "        timestep = 0\n",
    "        \n",
    "        while True:\n",
    "            timestep += 1\n",
    "            \n",
    "            x_t = cv2.cvtColor(cv2.resize(x_t, (84, 84)), cv2.COLOR_BGR2GRAY)\n",
    "            ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\n",
    "#             x_t = np.array([0.0 if (x//255) < 1 else 255.0 for x in x_t])\n",
    "            \n",
    "#             x_t = np.array(x_t, dtype=np.double)  # image value must be float cause net needs the values to calculate gradients\n",
    "            x_t = x_t.copy().astype(np.float32)\n",
    "            x_t = torch.from_numpy(x_t).unsqueeze(0)\n",
    "            x_t = x_t.unsqueeze(0)\n",
    "            \n",
    "            print(\"time \", timestep)\n",
    "            act_num = get_action(x_t, policy_net)\n",
    "            act = get_act_array(act_num)\n",
    "            x_t1, r_1, terminal = env.frame_step(act)\n",
    "            \n",
    "            memory.push(x_t, act_num, r_1, terminal)\n",
    "            \n",
    "            x_t = x_t1\n",
    "            \n",
    "            if memory.can_sample(BATCH_SIZE):\n",
    "                obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = memory.sample(BATCH_SIZE)\n",
    "                \n",
    "                curr_qs = policy_net(obs_batch).gather(1, act_batch)\n",
    "                next_qs = get_next_qs(target_net, next_obs_batch, done_mask, BATCH_SIZE)\n",
    "                \n",
    "                target_q_values = rew_batch + GAMMA * next_qs\n",
    "                \n",
    "                criterion = nn.MSELoss()\n",
    "                loss = critetion(curr_qs, target_q_values.unsqueeze(1))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            if terminal:\n",
    "                episode_durations.append(timestep)\n",
    "                plot_durations(episode_durations, 2)\n",
    "                break\n",
    "                \n",
    "            if episode % target_update == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "                \n",
    "        env.reinit()\n",
    "                \n",
    "            \n",
    "                \n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "3a323dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time  1\n",
      "time  2\n",
      "time  3\n",
      "time  4\n",
      "time  5\n",
      "time  6\n",
      "time  7\n",
      "time  8\n",
      "time  9\n",
      "time  10\n",
      "time  11\n",
      "time  12\n",
      "time  13\n",
      "time  14\n",
      "time  15\n",
      "time  16\n",
      "time  17\n",
      "time  18\n",
      "time  19\n",
      "time  20\n",
      "torch.Size([1, 32, 20, 20])\n",
      "act size torch.Size([1, 6])\n",
      "tensor([[ 0.5048, -1.1798,  0.1943, -0.6648, -0.4011,  0.5785]])\n",
      "act num  torch.Size([])\n",
      "time  21\n",
      "torch.Size([1, 32, 20, 20])\n",
      "act size torch.Size([1, 6])\n",
      "tensor([[ 0.7901, -1.0605,  0.2006, -0.7106, -0.4443,  0.4773]])\n",
      "act num  torch.Size([])\n",
      "time  22\n",
      "time  23\n",
      "torch.Size([1, 32, 20, 20])\n",
      "act size torch.Size([1, 6])\n",
      "tensor([[ 0.9579, -0.8202, -0.2388, -1.0575, -1.1393,  0.7303]])\n",
      "act num  torch.Size([])\n",
      "time  24\n",
      "time  25\n",
      "time  26\n",
      "time  27\n",
      "time  28\n",
      "time  29\n",
      "time  30\n",
      "ind  [15, 76, 94, 58, 4, 38, 0, 27, 49, 16, 43, 70, 89, 12, 18, 21, 81, 121, 33, 98, 51, 71, 119, 23, 13, 113, 114, 120, 73, 83, 127, 52, 53, 122, 26, 93, 50, 125, 75, 80, 90, 20, 100, 7, 28, 126, 72, 101, 111, 61, 128, 54, 62, 77, 30, 5, 17, 55, 103, 92, 24, 66, 2, 32, 31, 88, 41, 110, 74, 42, 8, 109, 91, 85, 102, 46, 65, 97, 86, 82, 10, 56, 78, 104, 87, 25, 3, 39, 47, 99, 57, 84, 68, 1, 48, 67, 11, 14, 116, 112, 79, 34, 64, 44, 19, 95, 37, 108, 118, 123, 69, 45, 29, 115, 96, 106, 22, 60, 9, 59, 36, 6, 107, 40, 35, 124, 117, 105]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "deque index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-286-f30e46717b61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGameState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-285-b29dd2c49d98>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, num_actions, in_channels, memory_size, screen_shape, target_update, BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, lr, num_episodes)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mobs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mcurr_qs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-282-3b696886d4b5>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ind \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-282-3b696886d4b5>\u001b[0m in \u001b[0;36m_encode_sample\u001b[0;34m(self, idxes)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mact_batch\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mrew_batch\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mnext_obs_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mdone_mask\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-282-3b696886d4b5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mact_batch\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mrew_batch\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mnext_obs_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscreens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mdone_mask\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: deque index out of range"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "num_actions = 6\n",
    "in_channels = 1\n",
    "screen_shape = (84, 84)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.001\n",
    "TARGET_UPDATE = 10\n",
    "lr = 0.001\n",
    "memory_size = 100000\n",
    "num_episodes = 1000\n",
    "\n",
    "env = game.GameState()\n",
    "train(env, num_actions, 1, memory_size, screen_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb803ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41067d28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
