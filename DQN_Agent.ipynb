{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e088f10",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7567320f5dde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#from utils.Replay.ipynb import ReplayBuffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_all_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m# import Game.tetris_fun as game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtetris_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.plot'"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "import gym, math, glob\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "# from utils.wrappers import *\n",
    "# from agents.DQN import Model as DQN_Agent\n",
    "# from utils.ReplayMemory import ExperienceReplayMemory\n",
    "\n",
    "#from utils.Replay.ipynb import ReplayBuffer\n",
    "# from utils.hyperparameters import Config\n",
    "# from utils.plot import plot_all_data\n",
    "# import Game.tetris_fun as game\n",
    "from Game.tetris_env import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ff4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for error: no available video device\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15033272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, size, screen_shape=(84, 84), frame_stack=True):\n",
    "        self.size = size\n",
    "        self.screen_shape = screen_shape\n",
    "        self.num_in_buffer = 0\n",
    "        self.memory = deque(maxlen=self.size)\n",
    "#         self.actions = deque(maxlen=self.size)\n",
    "#         self.rewards = deque(maxlen=self.size)\n",
    "# #          self.next_screens = deque(maxlin=self.size)\n",
    "#         self.terminal = deque(maxlen=self.size)\n",
    "        \n",
    "    def push(self, screen, action, reward, next_state, terminal):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        terminal = 0.0 if not terminal else 1.0\n",
    "        screen = torch.from_numpy(screen).to(dtype=torch.float32, device=device)\n",
    "#         print(\"action type\", (action, type(action[0])))\n",
    "        next_state = torch.from_numpy(next_state).to(dtype=torch.float32, device=device)\n",
    "        action = action.to(device)\n",
    "        reward = torch.tensor(reward).to(device)\n",
    "        terminal = torch.tensor(terminal).to(device)\n",
    "        self.memory.append((screen, action, reward, next_state, terminal))\n",
    "#         self.actions.append(action)\n",
    "#         self.rewards.append(reward)\n",
    "#         self.terminal.append(terminal)\n",
    "#         self.next_screens.append(next_screens)\n",
    "        \n",
    "        self.num_in_buffer = len(self.memory)\n",
    "#         print(\"buffer num \", self.num_in_buffer)\n",
    "#         print(\"action num \", len(self.actions))\n",
    "#         print(\"reward num \", len(self.rewards))\n",
    "#         print(\"ter num \", len(self.terminal))\n",
    "#         print(self.actions)\n",
    "        \n",
    "    def can_sample(self, batch_size):\n",
    "        \"\"\"Returns true if `batch_size` different transitions can be sampled from the buffer.\"\"\"\n",
    "        return batch_size + 1 <= self.num_in_buffer\n",
    "    \n",
    "    def _encode_sample(self, idxes):\n",
    "        # Return batch data for screens, actions, rewards, next screens and terminal info\n",
    "        # one screen state corresponding to one action by default, needing to consider grouped screens and actions\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        samples = [self.memory[i] for i in idxes]\n",
    "        obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = zip(*samples)  # unzip the samples and return tuples\n",
    "        print(\"sample zipped\", obs_batch[0])\n",
    "        print(\"act\", act_batch[0])\n",
    "        print(\"rew\", rew_batch[0])\n",
    "        print(\"next\", next_obs_batch[0])\n",
    "        print(\"done\",done_mask)\n",
    "        obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = map(torch.stack, zip(*samples))  # convert tuple into tensor form\n",
    "        print(\"obs_batch size\", obs_batch.shape)\n",
    "        print(\"act batch size\", act_batch.shape)\n",
    "        print(\"next b size\", next_obs_batch.shape)\n",
    "        print(done_mask)\n",
    "        \n",
    "#         obs_batch      = torch.from_numpy(np.concatenate([self.screens[idx] for idx in idxes], 0)).to(device)\n",
    "#         act_batch      = torch.tensor([[self.actions[idx]] for idx in idxes]).type(torch.long).to(device)\n",
    "#         rew_batch      = torch.tensor([self.rewards[idx] for idx in idxes]).to(device)\n",
    "#         next_obs_batch = torch.from_numpy(np.concatenate([self.screens[idx + 1] for idx in idxes], 0)).to(device)\n",
    "#         done_mask      = np.array([1.0 if self.terminal[idx] else 0.0 for idx in idxes], dtype=np.float32)\n",
    "        \n",
    "        return obs_batch, act_batch, rew_batch, next_obs_batch, done_mask\n",
    "        \n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "#         assert self.can_sample(batch_size)\n",
    "        inds = random.sample(range(self.num_in_buffer), batch_size)  # avoid sampling the last item and leading to index out of range for next state\n",
    "#         print(\"ind \", inds)\n",
    "        \n",
    "        return self._encode_sample(inds)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d6473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DQN model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_actions, in_channels=4, screen_shape=(84, 84)):\n",
    "        super(DQN, self).__init__()\n",
    "        in_channels = in_channels\n",
    "        num_actions = num_actions\n",
    "        screen_shape = screen_shape\n",
    "        h = screen_shape[0]\n",
    "        w = screen_shape[1]\n",
    "        \n",
    "        \n",
    "        # could add batchnorm2d layers after each covnet if data volume is too large\n",
    "        # see: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        \n",
    "        convw = self.conv2d_size_out(self.conv2d_size_out(self.conv2d_size_out(w, 8, 4), 4, 2), 3, 1)\n",
    "        convh = self.conv2d_size_out(self.conv2d_size_out(self.conv2d_size_out(h, 8, 4), 4, 2), 3, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(convw * convh * 64, 512)\n",
    "        self.fc2 = nn.Linear(512, num_actions)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "#         print(\"forward\", x.size())\n",
    "        x = F.relu(x)\n",
    "#         x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "#         print(\"forward\", x.size())\n",
    "        x = F.relu(self.conv3(x))\n",
    "#         print(\"fc size in\", x.size())\n",
    "        # reshape the tensor to one dimension for fc layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "#         print(\"fc size in\", x.size())\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def conv2d_size_out(self, size, kernel_size, stride, padding_size=0):\n",
    "        return (size + 2 * padding_size - (kernel_size - 1) - 1) // stride  + 1\n",
    "    \n",
    "\n",
    "# m = DQN(6, 1, (84, 84))\n",
    "# input1 = torch.randn(1, 1, 84, 84)\n",
    "# print(input1.dtype)\n",
    "# output = m(input1)\n",
    "\n",
    "# x_t = np.array([1, 2, 255, 3, 8, 255], dtype=np.double)\n",
    "# x_t = np.array([0.0 if x//255 < 1 else 255.0 for x in x_t])\n",
    "# print(x_t)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b6dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and utilities\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.001\n",
    "TARGET_UPDATE = 10\n",
    "lr = 0.001\n",
    "memory_size = 100000\n",
    "num_episodes = 1000\n",
    "\n",
    "\n",
    "def get_action(state, policy_net):\n",
    "    # Return a number indicating the pos of 1 in the array for a action\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    steps_done = 0\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            \n",
    "            state = torch.from_numpy(state)\n",
    "            if state.ndim < 4:\n",
    "                state = state.unsqueeze(0)\n",
    "            action = policy_net(state.to(device, dtype=torch.float32))\n",
    "#             print(\"act type\", type(action))\n",
    "#             print(action)\n",
    "#             action = action.max(1)[1][0]\n",
    "            action = action.max(1)[1]\n",
    "#             print(\"act num size\", action.size())\n",
    "            return action\n",
    "    else:\n",
    "        action = torch.tensor([random.randint(0, 5)])\n",
    "#         print(\"rand int size \", action.size())\n",
    "        return action\n",
    "    \n",
    "# def get_act_array(act_num):\n",
    "#     act_num = int(act_num.item())\n",
    "# #     print(\"act_num\", act_num)\n",
    "#     action = np.zeros(6, dtype=int)\n",
    "#     action[act_num] = 1\n",
    "#     return action\n",
    "\n",
    "def get_next_qs(target_net, next_obs_batch, done_mask, BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Return the Q-value of the next state.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    terminal_ind = torch.nonzero(done_mask)\n",
    "    values = target_net(next_obs_batch.to(device)).max(dim=1)[0].detach() \n",
    "#     print(\"values\", values.shape)\n",
    "#     print(terminal_ind)\n",
    "#     print(\"values val\", values)\n",
    "    values[terminal_ind] = 0.0\n",
    "#     print(\"values with ter\", values)\n",
    "    \n",
    "    \n",
    "#     not_terminal = np.where(done_mask==0.0)  # find elements fulfill the condition(==0.0), and return a tuple with an array of their positions\n",
    "#     not_terminal_states = next_obs_batch[not_terminal]\n",
    "#     values = torch.zeros(BATCH_SIZE).to(device)\n",
    "#     values[not_terminal] = target_net(not_terminal_states.to(device)).max(dim=1)[0].detach()\n",
    "    return values\n",
    "            \n",
    "# No need due to Wrapper added\n",
    "def preprocess(image, shape=(84,84)):\n",
    "    x_t = image\n",
    "    x_t = cv2.cvtColor(cv2.resize(x_t, shape), cv2.COLOR_BGR2GRAY)\n",
    "    ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\n",
    "    x_t = x_t.copy().astype(np.float32) # image value must be float cause net needs the values to calculate gradients\n",
    "    x_t = torch.from_numpy(x_t).unsqueeze(0)  # add one dimension representing the color channel\n",
    "    return x_t.unsqueeze(0)  # add one dimension to form a batch\n",
    "\n",
    "def plot_results(episode, episode_scores, losses, num_episodes, timestep):\n",
    "    display.clear_output(wait = True)\n",
    "    episode += 1\n",
    "    f, ax = plt.subplots(1, 2, figsize=(12, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Scores: %s, episode %s\" % (episode_scores[episode-1], episode))\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.xlim((0, num_episodes))\n",
    "    plt.ylim(bottom=-0.2)\n",
    "    plt.plot(episode_scores)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Losses: timestep %s\" % timestep)\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_durations(episode_durations, average_num=100):\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= average_num:\n",
    "        means = durations_t.unfold(0, average_num, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(average_num-1), means))\n",
    "        plt.plot(means.numpy())\n",
    "        \n",
    "    plt.show()\n",
    "#     plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "#     if is_ipython:\n",
    "#     display.clear_output(wait=True)\n",
    "#     display.display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d96f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.001\n",
    "TARGET_UPDATE = 10\n",
    "lr = 0.001\n",
    "memory_size = 100000\n",
    "num_episodes = 1000\"\"\"\n",
    "\n",
    "\n",
    "def train(env, num_actions, in_channels, memory_size=100000, screen_shape=(84, 84), target_update=10, \n",
    "          BATCH_SIZE=128, GAMMA=0.999, EPS_START=0.9, EPS_END=0.05, EPS_DECAY=0.001, lr=0.001, num_episodes=1000,\n",
    "         save_point=[500, 1000, 3000, 5000, 10000, 50000, 100000]):\n",
    "    env = env\n",
    "    saving_path = './model_saving'\n",
    "    save_point = save_point\n",
    "    \n",
    "    # if GPU is available, use it otherwise use CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    policy_net = DQN(num_actions, in_channels, screen_shape).to(device)\n",
    "    target_net = DQN(num_actions, in_channels, screen_shape).to(device)\n",
    "    \n",
    "    # set weight and bias of target net as policy net\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    \n",
    "    optimizer = optim.Adam(params=policy_net.parameters(), lr=lr)\n",
    "    \n",
    "    memory = ReplayBuffer(memory_size, screen_shape)\n",
    "    \n",
    "    episode_durations = []\n",
    "    scores = []\n",
    "    losses = []\n",
    "    last_screen = []\n",
    "    timestep = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "#         start_act = np.zeros(num_actions)\n",
    "#         x_t, r, _ = env.frame_step(start_act)\n",
    "#         x_t = preprocess(x_t, screen_shape)\n",
    "#         print(x_t.shape)\n",
    "#         x_t0, r0, terminal = env.frame_step(start_act)\n",
    "#         x_t0 = preprocess(x_t0, screen_shape)\n",
    "#         state = x_t0 - x_t\n",
    "#         last_screen.append(x_t)\n",
    "        \n",
    "        \n",
    "#         x_t = cv2.cvtColor(cv2.resize(x_t, (84, 84)), cv2.COLOR_BGR2GRAY)  # resize the screen and convert color to gray\n",
    "#         ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)  # set the background to black and tetriminos to white\n",
    "# #         s_t = np.stack((x_t, x_t, x_t, x_t), axis = 2)\n",
    "#         print(\"episode\", episode)\n",
    "        survive_points = 0\n",
    "        score = 0\n",
    "        x_t0 = env.reset()\n",
    "#         print(\"x_t0 size\", x_t0.shape)\n",
    "        \n",
    "        while True:\n",
    "            timestep += 1\n",
    "            \n",
    "#             x_t = cv2.cvtColor(cv2.resize(x_t, (84, 84)), cv2.COLOR_BGR2GRAY)\n",
    "#             ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\n",
    "# #             x_t = np.array([0.0 if (x//255) < 1 else 255.0 for x in x_t])\n",
    "            \n",
    "# #             x_t = np.array(x_t, dtype=np.double)  # image value must be float cause net needs the values to calculate gradients\n",
    "#             x_t = x_t.copy().astype(np.float32)\n",
    "#             x_t = torch.from_numpy(x_t).unsqueeze(0)\n",
    "#             x_t0, r_0, terminal = env.frame_step(start_act)\n",
    "#             x_t0 = preprocess(x_t0, (84, 84))\n",
    "#             x_t = last_screen[0]\n",
    "            \n",
    "#             print(\"time \", timestep)\n",
    "\n",
    "            act = get_action(x_t0, policy_net)\n",
    "#             print(\"int act\", int(act))\n",
    "            x_t1, r_1, terminal = env.step(int(act))\n",
    "#             print(\"terminal\", type(terminal))\n",
    "            \n",
    "        \n",
    "#             cv2.imwrite(\"frame\"+str(timestep)+\".png\", x_t1)\n",
    "            \n",
    "#             x_t1 = preprocess(x_t1, screen_shape)\n",
    "            \n",
    "            # Add extra reward if the agent survive\n",
    "            score += r_1\n",
    "            if not terminal:\n",
    "                r_1 += 0.1\n",
    "            \n",
    "            \n",
    "#             next_state = x_t1 - x_t0\n",
    "            \n",
    "            memory.push(x_t0, act, r_1, x_t1, terminal)\n",
    "            \n",
    "            print(\"score\", score)\n",
    "            \n",
    "            x_t0 = x_t1\n",
    "            \n",
    "            \n",
    "            if memory.can_sample(BATCH_SIZE):\n",
    "                obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = memory.sample(BATCH_SIZE)\n",
    "#                 print(\"obs \", obs_batch.size())\n",
    "#                 print(\"act \", type(act_batch))\n",
    "#                 print(\"rew \", rew_batch.size())\n",
    "                \n",
    "                curr_qs = policy_net(obs_batch.to(device))\n",
    "#                 print(\"curr size \", curr_qs.size())\n",
    "                curr_qs = policy_net(obs_batch.to(device)).gather(1, act_batch.to(device))  # retrieve the max Q-value according to the index specified in act-batch\n",
    "                next_qs = get_next_qs(target_net, next_obs_batch, done_mask, BATCH_SIZE)\n",
    "#                 print(\"next qs \", next_qs.size())\n",
    "#                 print(\"current qs \", curr_qs.size())\n",
    "#                 print(curr_qs[0])\n",
    "                \n",
    "                target_q_values = rew_batch + GAMMA * next_qs\n",
    "#                 print(\"target q \", target_q_values.size())\n",
    "                \n",
    "                criterion = nn.MSELoss()\n",
    "                loss = criterion(curr_qs, target_q_values.unsqueeze(1))\n",
    "                losses.append(loss)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update target net at even interval \n",
    "                if timestep % target_update == 0:\n",
    "                    target_net.load_state_dict(policy_net.state_dict())\n",
    "                \n",
    "            if terminal:\n",
    "                episode_durations.append(timestep)\n",
    "                scores.append(score)\n",
    "                print(scores)\n",
    "                plot_results(episode, scores, losses, num_episodes)\n",
    "                plot_durations(episode_durations)\n",
    "                break\n",
    "                \n",
    "            \n",
    "                \n",
    "            if episode in save_point:\n",
    "                torch.save(policy_net, \"%s/%s_%s.pth\" % (saving_path, \"DQN\", episode))\n",
    "                torch.save({\n",
    "                            'episode': episode,\n",
    "                            'model_state_dict': policy_net.state_dict(),\n",
    "                            'target_state_dict': target_net.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'loss': loss,\n",
    "                            }, \"%s/%s_%s_train.pth\" % (saving_path, \"DQN\", episode))   # save for later training\n",
    "                \n",
    "        \n",
    "                \n",
    "            \n",
    "                \n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85090409",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 6\n",
    "in_channels = 4  # due to frame stack\n",
    "screen_shape = (84, 84)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 0.001\n",
    "TARGET_UPDATE = 100\n",
    "FRAMESKIP = 4\n",
    "lr = 0.001\n",
    "memory_size = 100000\n",
    "num_episodes = 50000\n",
    "\n",
    "env = TetrisEnv()\n",
    "env = TetrisPreprocessing(env, screen_size=84)\n",
    "env = FrameStack(env,4)\n",
    "# env.reset()\n",
    "# o, r, d = env.step(1)\n",
    "# print(o, r, d)\n",
    "# y, r, t = env.frame_step(np.array([0, 0, 1, 0, 0, 0]))\n",
    "# print(y)\n",
    "# x = env.get_pixelscreen()\n",
    "# env.save_screen()\n",
    "train(env, num_actions, in_channels, memory_size, screen_shape, target_update = TARGET_UPDATE, num_episodes=num_episodes, save_point=[200, 300, 400, 500, 600, 700, 800, 900, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc294928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.zeros([6])\n",
    "# assert x.size == 7, (x.size, 7) \n",
    "class testing:\n",
    "    def __init__(self):\n",
    "        self.test = 123\n",
    "        self.key = 'testing'\n",
    "    def __array__(self):\n",
    "        arr = self[:]\n",
    "        print(arr)\n",
    "    def __getitem__(self, item):\n",
    "         return self.key\n",
    "y = testing()\n",
    "print(dir(y))\n",
    "y.__array__()\n",
    "z = y[:]\n",
    "print(type(z))\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "np.array(np.stack((a, b)))\n",
    "\n",
    "x = np.array([[1,2],[3,4]])\n",
    "np.repeat(x, [2, 2], axis=0)\n",
    "l = [9.8] * 3\n",
    "x[np.newaxis, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f328ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47737653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
