{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf1b530",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5f3d3df24093>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtimeit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefault_timer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import gym, math, glob\n",
    "import numpy as np\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "from utils.wrappers import *\n",
    "from agents.DQN import Model as DQN_Agent\n",
    "from utils.ReplayMemory import ExperienceReplayMemory\n",
    "\n",
    "from utils.hyperparameters import Config\n",
    "from utils.plot import plot_all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b836a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Model):\n",
    "    def __init__(self, in_channels, num_actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(self.in_channels[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        self.adv1 = nn.Linear(self.feature_size(), 512)\n",
    "        self.adv2 = nn.Linear(512, self.num_actions)\n",
    "\n",
    "        self.val1 = nn.Linear(self.feature_size(), 512)\n",
    "        self.val2 = nn.Linear(512, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        adv = F.relu(self.adv1(x))\n",
    "        adv = self.adv2(adv)\n",
    "\n",
    "        val = F.relu(self.val1(x))\n",
    "        val = self.val2(val)\n",
    "\n",
    "        return val + adv - adv.mean()\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.conv3(self.conv2(self.conv1(torch.zeros(1, *self.in_channels)))).view(1, -1).size(1)\n",
    "    \n",
    "    def sample_noise(self):\n",
    "        #ignore this for now\n",
    "        pass\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b36671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(DQN_Agent):\n",
    "    def __init__(self, static_policy=False, env=None, config=None, log_dir='/tmp/gym'):\n",
    "        super(Model, self).__init__(static_policy, env, config, log_dir=log_dir)\n",
    "\n",
    "    def declare_networks(self):\n",
    "        self.model = DuelingDQN(self.env.observation_space.shape, self.env.action_space.n)\n",
    "        self.target_model = DuelingDQN(self.env.observation_space.shape, self.env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64817e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=timer()\n",
    "\n",
    "log_dir = \"/tmp/gym/\"\n",
    "try:\n",
    "    os.makedirs(log_dir)\n",
    "except OSError:\n",
    "    files = glob.glob(os.path.join(log_dir, '*.monitor.csv')) \\\n",
    "        + glob.glob(os.path.join(log_dir, '*td.csv')) \\\n",
    "        + glob.glob(os.path.join(log_dir, '*sig_param_mag.csv')) \\\n",
    "        + glob.glob(os.path.join(log_dir, '*action_log.csv'))\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "\n",
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env    = make_atari(env_id)\n",
    "env    = bench.Monitor(env, os.path.join(log_dir, env_id))\n",
    "env    = wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=True)\n",
    "env    = WrapPyTorch(env)\n",
    "model  = Model(env=env, config=config, log_dir=log_dir)\n",
    "\n",
    "episode_reward = 0\n",
    "\n",
    "observation = env.reset()\n",
    "for frame_idx in range(1, config.MAX_FRAMES + 1):\n",
    "    epsilon = config.epsilon_by_frame(frame_idx)\n",
    "\n",
    "    action = model.get_action(observation, epsilon)\n",
    "    model.save_action(action, frame_idx) #log action selection\n",
    "\n",
    "    prev_observation=observation\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    observation = None if done else observation\n",
    "\n",
    "    model.update(prev_observation, action, reward, observation, frame_idx)\n",
    "    episode_reward += reward\n",
    "\n",
    "    if done:\n",
    "        model.finish_nstep()\n",
    "        model.reset_hx()\n",
    "        observation = env.reset()\n",
    "        model.save_reward(episode_reward)\n",
    "        episode_reward = 0\n",
    "\n",
    "    if frame_idx % 10000 == 0:\n",
    "        model.save_w()\n",
    "        try:\n",
    "            clear_output(True)\n",
    "            plot_all_data(log_dir, env_id, 'DuelingDQN', config.MAX_FRAMES, bin_size=(10, 100, 100, 1), smooth=1, time=timedelta(seconds=int(timer()-start)), ipynb=True)\n",
    "        except IOError:\n",
    "            pass\n",
    "\n",
    "model.save_w()\n",
    "env.close()\n",
    "plot_all_data(log_dir, env_id, 'DuelingDQN', config.MAX_FRAMES, bin_size=(10, 100, 100, 1), smooth=1, time=timedelta(seconds=int(timer()-start)), ipynb=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
